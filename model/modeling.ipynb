{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f38af0c3-401c-4067-aa64-a32f1737cb9c",
   "metadata": {},
   "source": [
    "# Deep Learning Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e57d9c2d-8bd9-4e8d-a74d-2b36c392cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "#from PIL import Image\n",
    "# !pip install mat73\n",
    "import mat73\n",
    "import helper\n",
    "\n",
    "#import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, LeakyReLU\n",
    "from tensorflow.keras.layers import Dense, Flatten, Concatenate\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4dd1ae-5eb1-4a67-b9cd-b1514e9d43a3",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca4e2cea-dfe1-45d7-afef-d0df81bef786",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a4d078-796a-4478-81b2-1786e9e01c04",
   "metadata": {},
   "source": [
    "## Image loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c88ca7e-a8d3-4e41-92e8-6317fe7ba3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load each file independently (to avoid memory overflows) and retriev PID\n",
    "patient_ids_dict = dict()\n",
    "label_ids_dict = dict()\n",
    "shape_x_dict = dict()\n",
    "shape_y_dict = dict()\n",
    "image_dict = dict()\n",
    "\n",
    "for file_number in range(1,3047):\n",
    "    file = f'../data/brain-tumor-data-public/{file_number}.mat'\n",
    "    data_dict = mat73.loadmat(file)\n",
    "    patient_ids_dict[file_number] = data_dict['cjdata']['PID']\n",
    "    label_ids_dict[file_number] = int(data_dict['cjdata']['label'])\n",
    "    shape_x_dict[file_number] = data_dict['cjdata']['image'].shape[0]\n",
    "    shape_y_dict[file_number] = data_dict['cjdata']['image'].shape[1]\n",
    "    image_dict[file_number] = data_dict['cjdata']['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30688f85-068d-4622-b940-9a492b36901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = pd.Series(patient_ids_dict)\n",
    "label_ids = pd.Series(label_ids_dict)\n",
    "shape_x = pd.Series(shape_x_dict)\n",
    "shape_y = pd.Series(shape_y_dict)\n",
    "image = pd.Series(image_dict)\n",
    "\n",
    "patients = pd.DataFrame({'pid':patient_ids, 'label':label_ids,\n",
    "                         'x':shape_x, 'y':shape_y, 'image':image}, index=range(1, 3047))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d8b0d8-bd61-46c9-abf8-db140e36cdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients['image'] = patients['image'].apply(lambda x : helper.imx_preproc(x, zero_up_to_one=True, resize=resize))\n",
    "patients['image'] = patients['image'].apply(lambda x : x.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7870dbe9-dbc7-430e-a06a-4d2dd49506a0",
   "metadata": {},
   "source": [
    "## Train and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea12e367-d931-4bea-b7da-c8b2fdc7fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "val_size = 0.2\n",
    "test_size = 0.2\n",
    "# Training set has 2436 images test set has 610 images \n",
    "train_set, test_set = train_test_split(patients, stratify=patients['label'], test_size=test_size)\n",
    "# Training set has 1948 images, validation set has 488 images\n",
    "train_set, val_set = train_test_split(train_set, stratify=train_set['label'], test_size=val_size)\n",
    "\n",
    "n_total_train = train_set.shape[0]\n",
    "\n",
    "train_set_x = np.stack(train_set['image'].values)\n",
    "train_set_y = np.array(train_set['label'])\n",
    "\n",
    "val_set_x = np.stack(val_set['image'].values)\n",
    "val_set_y = np.array(val_set['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051f066f-58b9-4b7a-9625-03b5e0b0221d",
   "metadata": {},
   "source": [
    "Applying one hot encoding to target variable y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "012d02e7-7255-41eb-bb7a-35e5aacee3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 18:40:19.009265: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-29 18:40:19.009615: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-29 18:40:19.010877: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "new_train_set_y = tf.one_hot(\n",
    "    train_set_y,\n",
    "    4,\n",
    "    on_value=None,\n",
    "    off_value=None,\n",
    "    axis=None,\n",
    "    dtype=None,\n",
    "    name=None\n",
    ")\n",
    "new_train_set_y = np.array(new_train_set_y)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ab7b885-9d03-49e9-92b6-57d576d547bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_val_set_y = tf.one_hot(\n",
    "    val_set_y,\n",
    "    4,\n",
    "    on_value=None,\n",
    "    off_value=None,\n",
    "    axis=None,\n",
    "    dtype=None,\n",
    "    name=None\n",
    ")\n",
    "new_val_set_y = np.array(new_val_set_y)[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d524290d-d579-4993-9901-d09cc1af50f1",
   "metadata": {},
   "source": [
    "## CNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d8ae5-f5e1-42fd-b342-8a56e523515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "IMG_H = int(round(512 * resize, 0))\n",
    "IMG_W = int(round(512 * resize, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c6311-b7e8-4e5e-a5ca-13891d27cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = Sequential([\n",
    "    InputLayer(input_shape=(IMG_H, IMG_W, 1)),\n",
    "    \n",
    "    Conv2D(64, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "\n",
    "    Conv2D(64, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "\n",
    "    Conv2D(64, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "\n",
    "    Conv2D(64, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4772e8-ebb3-44f7-8c9d-ec3d7f27762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile: Define training parameters\n",
    "CNN.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b120611c-6311-46b6-b076-7cd2b22d62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "EPOCHS = 10\n",
    "BATCHES = n_total_train//batch_size\n",
    "\n",
    "### Following line overflows memory\n",
    "CNN.fit(train_set_x, new_train_set_y, steps_per_epoch=BATCHES, epochs=EPOCHS, validation_data=(val_set_x, new_val_set_y))\n",
    "\n",
    "### Following line can still not be run because we do not have images as jpg in specific folders\n",
    "# CNN.fit(train_generator, steps_per_epoch=BATCHES, epochs=EPOCHS, validation_data=val_generator)\n",
    "\n",
    "### Following line was thought to be a solution to the memory running out, but it wasn't\n",
    "# CNN.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc45c2b-ed64-4e72-b9ea-8edfc39f38b0",
   "metadata": {},
   "source": [
    "This alternative does not require doing OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b11f3a-cca3-4252-a38f-7936fe014919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile: Define training parameters\n",
    "CNN.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef61f5b4-a0fc-4515-832d-5af5858c74b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "EPOCHS = 10\n",
    "BATCHES = n_total_train//batch_size\n",
    "\n",
    "### Following line overflows memory\n",
    "CNN.fit(train_set_x, new_train_set_y, steps_per_epoch=BATCHES, epochs=EPOCHS, validation_data=(val_set_x, new_val_set_y))\n",
    "\n",
    "### Following line can still not be run because we do not have images as jpg in specific folders\n",
    "# CNN.fit(train_generator, steps_per_epoch=BATCHES, epochs=EPOCHS, validation_data=val_generator)\n",
    "\n",
    "### Following line was thought to be a solution to the memory running out, but it wasn't\n",
    "# CNN.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf5b7b-60a2-4ff8-bbe4-5b9c74a6bcde",
   "metadata": {},
   "source": [
    "Esta es para que el validation set se calcule a partir del train set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b38f4977-6177-47cf-95f9-505cacdd8c15",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30892/1446984644.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mBATCHES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_total_train\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m### Following line overflows memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "EPOCHS = 10\n",
    "BATCHES = n_total_train//batch_size\n",
    "\n",
    "### Following line overflows memory\n",
    "CNN.fit(train_set_x, train_set_y, steps_per_epoch=BATCHES, epochs=EPOCHS, validation_split = 0.1)\n",
    "\n",
    "### Following line can still not be run because we do not have images as jpg in specific folders\n",
    "# CNN.fit(train_generator, steps_per_epoch=BATCHES, epochs=EPOCHS, validation_data=val_generator)\n",
    "\n",
    "### Following line was thought to be a solution to the memory running out, but it wasn't\n",
    "# CNN.fit(train_dataset, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
